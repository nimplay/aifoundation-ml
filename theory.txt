Supervised vs Unsupervised

Supervised:
 1. Requires training data with independent variables & a dependent variables (labelled data)
 2. Need labelled data to "supervise" the algorithm when learning from the data
 examples
  -Regression Models
  -Classification Models

Unsupervised:
 1. Requires training data with independent variables only
 2. Not Need labelled data that can "supervise" the algorithm when learning from data
 examples
  -Clustering Models
  -Outlier Detection Models

Regression vs Classification

Regression:
 1. Can be used when the response variable to be predicted is a continuous variable (scaler)
 examples:
  -Linear Regression, Fixed Effects Regression, XGBoost Regression

Classification:
 1. Can be used when the response variable to be predicted is a continuous variable (scaler)
 examples:
  -Logistic Regression, XGBoost Classification

Regression Performance Metrics

RRS (Residual Sum Square)
MSE (Mean Square Error)
RMSE (Root Mean Square Error)
MAE (Mean Absolute Error)

Classification Performance Metrics

Accuracy
Precision
Recall
F-1 Score

Clustering Performance Metrics

Homogeneity
Silhouette Score
Completeness

Training Machine Learning Model (Simplest version):

Step 1
    Data Preparation
      Split the data into train, validation and test.
Step 2
    Model Trainig
      Train the model on the training data and save the fitted model.
Step 3
    Hyper-Parameter Tuning
      Use the fitted Model and Validation Set to find the optimal set of parameters where the model performs the best.
Step 4
    Prediction
      Use the optimal set of parameters from Hyper-Parameter Tunning data, to train the model again with these hyeper parameters, use this best fitted model to predictions on test data.
Step 5
    Test Error Rate
      Compute the performance metrics for your model using the predictions and real values of the target variable from your test data.

Bias:

Bias of machine Learning model is its inibility to capture the true relationship the data, methemetically equal the
difference between the Expectation of model estimation and its true value

Variance:

Variance of Machine Learning model is the inconstancy level of model performance when applying it to different data Simplest
 -When the same model that is trained using training data performs entirely different than on test data then model variance is high

Error:

Error of Machine Learning model assuming model is trained on (X1, Y1) (X2, Y2)...(Xn, Yn) to estimate the value of Y0

Bias-Variance Trade-off:

In order to minimize the expected test error rate, we need to select a Machine Learning method that simultaneously
achieves low variance and low bias

  -Negative correlation between Variance and Bias of model
  -ML model's flexibility has direct impact on its Variance/Bias

Overfitting:

Overfitting occurs when the model performs well in the training (low train error rate) while performs worse on the test
data (high test error rate)

How to fix Overfitting?

 1. Reduce the complexity of model
 2. Collect more data
 3.  Use Resampling Technique (CV)
 4. Early Stopping
 5. Ensemble Methods
 6. Dropout

 Regularization or Shrinkage:

 Regularization or Shrinkage is a method that shrinks some of the estimated coefficiente towards zero,
 to penalize unimportant variables for increasing the variance of the model
  - Use solve the overfitting problem
  - Introduces a little bias in the model to increase its variance

  1. Ridge Regression based on L2 norm
  2. Lasso Regression based on L1 norm
  3. Dropout (in NN)

Ridge Regression or L2 regularization:

Ridge Regression or L2 regularization is a shrinkage technique that aims to solve
Overfitting by shrinking some of the model coefficients towards 0

L2 Norm and Ridge Regression:

L2 Norm is a mathematical term comin from linear algebra and it stands for Euclidean norm or distance


