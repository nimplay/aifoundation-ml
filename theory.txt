Supervised vs Unsupervised

Supervised:
 1. Requires training data with independent variables & a dependent variables (labelled data)
 2. Need labelled data to "supervise" the algorithm when learning from the data
 examples
  -Regression Models
  -Classification Models

Unsupervised:
 1. Requires training data with independent variables only
 2. Not Need labelled data that can "supervise" the algorithm when learning from data
 examples
  -Clustering Models
  -Outlier Detection Models

Regression vs Classification

Regression:
 1. Can be used when the response variable to be predicted is a continuous variable (scaler)
 examples:
  -Linear Regression, Fixed Effects Regression, XGBoost Regression

Classification:
 1. Can be used when the response variable to be predicted is a continuous variable (scaler)
 examples:
  -Logistic Regression, XGBoost Classification

Regression Performance Metrics

RRS (Residual Sum Square)
MSE (Mean Square Error)
RMSE (Root Mean Square Error)
MAE (Mean Absolute Error)

Classification Performance Metrics

Accuracy
Precision
Recall
F-1 Score

Clustering Performance Metrics

Homogeneity
Silhouette Score
Completeness

Training Machine Learning Model (Simplest version):

Step 1
    Data Preparation
      Split the data into train, validation and test.
Step 2
    Model Trainig
      Train the model on the training data and save the fitted model.
Step 3
    Hyper-Parameter Tuning
      Use the fitted Model and Validation Set to find the optimal set of parameters where the model performs the best.
Step 4
    Prediction
      Use the optimal set of parameters from Hyper-Parameter Tunning data, to train the model again with these hyeper parameters, use this best fitted model to predictions on test data.
Step 5
    Test Error Rate
      Compute the performance metrics for your model using the predictions and real values of the target variable from your test data.

Bias:

Bias of machine Learning model is its inibility to capture the true relationship the data, methemetically equal the
difference between the Expectation of model estimation and its true value

Variance:

Variance of Machine Learning model is the inconstancy level of model performance when applying it to different data Simplest
 -When the same model that is trained using training data performs entirely different than on test data then model variance is high

Error:

Error of Machine Learning model assuming model is trained on (X1, Y1) (X2, Y2)...(Xn, Yn) to estimate the value of Y0

Bias-Variance Trade-off:

In order to minimize the expected test error rate, we need to select a Machine Learning method that simultaneously
achieves low variance and low bias

  -Negative correlation between Variance and Bias of model
  -ML model's flexibility has direct impact on its Variance/Bias

Overfitting:

Overfitting occurs when the model performs well in the training (low train error rate) while performs worse on the test
data (high test error rate)

How to fix Overfitting?

 1. Reduce the complexity of model
 2. Collect more data
 3.  Use Resampling Technique (CV)
 4. Early Stopping
 5. Ensemble Methods
 6. Dropout

 Regularization or Shrinkage:

 Regularization or Shrinkage is a method that shrinks some of the estimated coefficiente towards zero,
 to penalize unimportant variables for increasing the variance of the model
  - Use solve the overfitting problem
  - Introduces a little bias in the model to increase its variance

  1. Ridge Regression based on L2 norm
  2. Lasso Regression based on L1 norm
  3. Dropout (in NN)

Ridge Regression or L2 regularization:

  Ridge Regression or L2 regularization is a shrinkage technique that aims to solve
  Overfitting by shrinking some of the model coefficients towards 0

  L2 Norm and Ridge Regression:
    L2 Norm is a mathematical term comin from linear algebra and it stands for Euclidean norm or distance

  Pros and cons

  Pros
  1 Solves overfitting
  2 Lower model variance
  3 Computationally cheap

  Cons
  1 Low interpretability

Lasso Regression or L1 regularization:
 It is a shrinkage technique that aims to solve Overfitting by shrinking
 some of the model coefficients towards 0 and setting some to exactly 0

 L1 Norm and Lasso Regression
   L1 Norm is a mathematical term coming from Linear Algebra and it stands for a
   Manhattan norm or distance

  Pros and Cons
   Pros
   1. Solves Overfitting
   2. Easy to understand
   3. High interpretability
   4. Feature selection

   Cons
   1. Higher variance than Ridge

Statistical Significant Effects:
  It means that this effecs is unlikely to have occurred by chance. In order words,
  a statistically significant effect is one that is likely to be real and not due to random chance

Linear Regression:
  It is a statistical method that can help to model the impact of a unit change in a
  variable(independent variable) on the values of another target variable(dependent variable),
  when the relationship between two variables is linear

   - 1 independent Variable => Simple Linear Regression
   - 2 or more Independent Variables => Multiple Linear Regression

  Linear Regression Estimating
    Ordinary Least Square(OLS):
      It is an estimation technique for estimating unknown parameters in a linear regression model
      to predict the response dependent variables(eg. B0,B1, etc)

  Linear Regression Assumptions
    OLS makes the following assumption which need to be satisfied to get a reliable prediction results:
      1. A1 Linearity: model is linear in parameters
      2. A2 Random Sample: all observations in the sample are randomly selected
      3. A3 Exogeneity: each independent variable is uncorrelated with the error terms
      4. A4 Homoscedasticity: variance of all error terms is constant
      5. A5 No Perfect MultiCollinearity: there are no exact linear relationship between the independent variables

  Pros
    1. Simple model
    2. Computationally efficient
    3. High interpretability
    4. Able to handle missing date

  Cons
    1. Overly-Simplisttic
    2. Many Assumptions
    3. Assumes Linearity
    4. Prone to Outliners

top 10 algorithms
  1.Linear Regression
      Examples:
        a. Real State Pricing
        b. Credit Scoring
        c. Supply Chain Costs
        d. Healthcare
        e. Academic Performance
        f. Energy Consumption
  2. Logistic Regression
      Examples:
        a. Medics Diagnostic
        b. Span Detection
        c. Digital Marketing
        d. Credit Risk
        e. Churn Prediction
  3. Linear Discriminant Analysis
      Examples:
        a. Medics Diagnostic
        b. Facial Reconigtion
        c. Digital Marketing
        d. Text Classification
        e. Agriculture of Presition
        f. Ecology
        g. Industrial Quality
        h. Genome
        i. Geology
        j. Finance
  4 Naive Bayes
    Examples:
        a. Natural Languaje Process
        b. Medics Diagnostic
        c. Marketing and Business
        d. Pattern Reconigtion
        e. Chatbots (Client Support System)
        f. Bioinformatics
        h. Risk Analysis
  5 Decision tree
    Examples:
        a. Medics Diagnostic
        b. Marketing and Business
        c. Finance
        d. Data Structure Processing
        e. Ecology
        f. Recomendation System
        g. Educational Data
  6 Bagging
    Examples:
        a. Classification and Regression Problems
        b. Images Processing  and Computing Vision
        c. Natural Language Processing
        d. Finance and Risk
        e. Maintain Predictive Industries
        f. Ecology and Environment Sciens
  7 Random Forest
    Examples:
        a. Medics Diagnostic
        b. Marketing and Business
        c. Natural Language Processing
        d. Finance and Bank
        e. Images Processing  and Computing Vision
        f. Ecology and Environment Sciens
        g. Informatic Security
  8 Boosting or Ensamble Technique(AdaBoost, GBM, XGBoost)
    Examples:
        a. Medics Diagnostic
        b. Marketing and Business
        c. Natural Language Processing
        d. Finance and Bank
        e. Images Processing  and Computing Vision
        f. Ecology and Environment Sciens
        g. Informatic Security

